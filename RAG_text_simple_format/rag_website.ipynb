{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc813032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document appears to be composed primarily of \"Lorem ipsum\" placeholder text, which is used to demonstrate the visual form of a document without conveying any meaningful content. Therefore, it does not have a main topic.\n"
     ]
    }
   ],
   "source": [
    "# RAG pipeline in a single cell using LangChain for PDF and Gemini\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Load PDF and extract text\n",
    "def load_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    return loader.load()\n",
    "\n",
    "# 2. Split text into chunks\n",
    "def split_docs(docs, chunk_size=400, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# 3. Embed and store in vector DB\n",
    "def create_vector_store(docs):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# 4. Build RAG QA chain with Gemini\n",
    "def build_qa_chain(vectorstore):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)  # Requires GOOGLE_API_KEY env variable\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    return qa_chain\n",
    "\n",
    "# 5. Main RAG pipeline\n",
    "def rag_pdf_qa(pdf_path, question):\n",
    "    docs = load_pdf(pdf_path)\n",
    "    chunks = split_docs(docs)\n",
    "    vectorstore = create_vector_store(chunks)\n",
    "    qa_chain = build_qa_chain(vectorstore)\n",
    "    answer = qa_chain.run(question)\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = r\"C:\\Users\\admin\\Desktop\\multimodel-rag-Customer_support\\RAG_text\\file-sample_150kB.pdf\"  # Replace with your PDF file path\n",
    "question = \"What is the main topic of this document?\"\n",
    "\n",
    "\n",
    "print(rag_pdf_qa(pdf_path, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04212d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0eac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5917f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " This document does not have a main topic. It is composed entirely of \"Lorem ipsum\" placeholder text, which is used to demonstrate the visual form of a document without conveying any meaningful content.\n",
      "\n",
      "Sources:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "### RAG with multiple pdf and gemini \n",
    "\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "\n",
    "# 1. Load one or many PDFs and extract text\n",
    "def load_pdf(pdf_paths):\n",
    "    if isinstance(pdf_paths, str):  # single path\n",
    "        pdf_paths = [pdf_paths]\n",
    "    docs = []\n",
    "    for path in pdf_paths:\n",
    "        loader = PyPDFLoader(path)\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "# 2. Split text into chunks\n",
    "def split_docs(docs, chunk_size=400, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "# # 3. Embed and store in vector DB\n",
    "# def create_vector_store(docs):\n",
    "#     embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#     vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "#     return vectorstore\n",
    "\n",
    "\n",
    "### Embedding with GEMINI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "def create_vector_store(docs):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "def build_qa_chain(vectorstore):\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"You are a helpful assistant. Use the following context to answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 4. Build RAG QA chain with Gemini\n",
    "# def build_qa_chain(vectorstore):\n",
    "#     if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#         raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\")\n",
    "    \n",
    "#     llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)\n",
    "#     retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "#     qa_chain = RetrievalQA.from_chain_type(\n",
    "#         llm=llm,\n",
    "#         retriever=retriever,\n",
    "#         chain_type=\"stuff\",\n",
    "#         return_source_documents=True\n",
    "#     )\n",
    "\n",
    "#     return qa_chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Main RAG pipeline\n",
    "def rag_pdf_qa(pdf_paths, question):\n",
    "    try:\n",
    "        docs = load_pdf(pdf_paths)\n",
    "\n",
    "        chunks = split_docs(docs)\n",
    "\n",
    "        vectorstore = create_vector_store(chunks)\n",
    "\n",
    "\n",
    "        qa_chain = build_qa_chain(vectorstore)\n",
    "\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "   \n",
    "        answer = result[\"result\"]\n",
    "        # sources = [doc.metadata for doc in result.get(\"source_documents\", [])]\n",
    "        return answer, sources\n",
    "    except Exception as e:\n",
    "        return f\"Error while processing: {e}\", []\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = r\"C:\\Users\\admin\\Desktop\\multimodel-rag-Customer_support\\RAG_text\\file-sample_150kB.pdf\"\n",
    "question = \"What is the main topic of this document?\"\n",
    "\n",
    "answer, sources = rag_pdf_qa(pdf_path, question)\n",
    "\n",
    "print(\"\\nAnswer:\\n\", answer)\n",
    "print(\"\\nSources:\\n\", sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fb7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "939e0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Self-reflection is a vital mechanism that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "\n",
      "Here's how it works and its purpose:\n",
      "*   **Purpose:** It enables agents to do self-criticism over past actions, learn from mistakes, and refine them for future steps, thereby improving the quality of final results and enhancing reasoning skills.\n",
      "*   **Creation:** It is created by showing two-shot examples to a Large Language Model (LLM). Each example is a pair consisting of a failed trajectory and an ideal reflection designed to guide future changes in the agent's plan.\n",
      "*   **Usage:** These generated reflections are then added into the agentâ€™s working memory, typically up to three, to be used as context for querying the LLM in subsequent steps.\n",
      "*   **Impact:** Based on the self-reflection results, an agent might compute a heuristic and optionally decide to reset the environment to start a new trial.\n",
      "*   **Frameworks:** Reflexion (Shinn & Labash 2023) is a framework specifically designed to equip agents with dynamic memory and self-reflection capabilities.\n",
      "*   **Benefit from Context:** Mechanisms like self-reflection, which learn from past mistakes, benefit significantly from long or infinite context windows.\n"
     ]
    }
   ],
   "source": [
    "## LANgChain with webpage\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 2. Embed and store in vector DB\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "# 3. Define a custom prompt (optional)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"You are a precise assistant. Use ONLY the context to answer.If unsure, say 'I don't know'.\\n\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# 4. Build the RAG QA chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.1)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    #chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# 5. Ask a question\n",
    "question = \"Explain Self-Reflection\"\n",
    "\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # get retrieved chunks\n",
    "# retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# print(\"\\n=== Retrieved Chunks ===\\n\")\n",
    "# for i, doc in enumerate(retrieved_docs, 1):\n",
    "#     print(f\"--- Chunk {i} ---\")\n",
    "#     print(doc.page_content[:500], \"...\")  # print first 500 chars for readability\n",
    "#     print(f\"Source: {doc.metadata}\\n\")\n",
    "\n",
    "# # run QA chain\n",
    "# result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "# print(\"\\n=== Answer ===\\n\")\n",
    "# print(result[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548acac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
